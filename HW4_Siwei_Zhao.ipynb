{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_4_problem_statement.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sppaIxalGpk3"
      },
      "source": [
        "# CS 6140 Machine Learning: Assignment - 4 (Total Points: 100)\n",
        "## Prof. Ahmad Uzair "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMVuUyGzG5ez"
      },
      "source": [
        "## Q1. Random Forest (50 points) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUnwBd9HnkrV"
      },
      "source": [
        "**Random forests**, also known as **Random Decision Forests**, are an ensemble learning method for classification, regression, and other problems that works by training a large number of **decision trees**. For classification tasks, the random forest's output is the class chosen by the majority of trees. The mean or average prediction of the individual trees is returned for regression tasks. In this assignment we will be using Random Forest Classifier to predict the possibility of breast cancer being benign or not.\n",
        "\n",
        "Note that the code for building a decision tree will be similar to your first assignment. Feel free to reuse the code from your first assignment but be ware of the differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YppAr1nZQYT"
      },
      "source": [
        "# Do not change anything in this cell.\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiDnN8cApVoN"
      },
      "source": [
        "We are using the breast cancer dataset provided in scikit-learn. Here is a look at your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LbsU8qEZoxI"
      },
      "source": [
        "# Do not change anything in this cell.\n",
        "data = datasets.load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AS6zNCJcqCN",
        "outputId": "fded15d0-92ce-4f82-fdd6-6545e9677811"
      },
      "source": [
        "# Do not change anything in this cell.\n",
        "data"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry \\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 3 is Mean Radius, field\\n        13 is Radius SE, field 23 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
              " 'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
              "         1.189e-01],\n",
              "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
              "         8.902e-02],\n",
              "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
              "         8.758e-02],\n",
              "        ...,\n",
              "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
              "         7.820e-02],\n",
              "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
              "         1.240e-01],\n",
              "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
              "         7.039e-02]]),\n",
              " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
              "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
              "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
              "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
              "        'smoothness error', 'compactness error', 'concavity error',\n",
              "        'concave points error', 'symmetry error',\n",
              "        'fractal dimension error', 'worst radius', 'worst texture',\n",
              "        'worst perimeter', 'worst area', 'worst smoothness',\n",
              "        'worst compactness', 'worst concavity', 'worst concave points',\n",
              "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
              " 'filename': '/usr/local/lib/python3.7/dist-packages/sklearn/datasets/data/breast_cancer.csv',\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
              "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " 'target_names': array(['malignant', 'benign'], dtype='<U9')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSzU9oQkdRDl"
      },
      "source": [
        "# Write a function to calculate the entropy. We will be using this to calculate\n",
        "## the information gain for decision tree.\n",
        "def entropy(y):\n",
        "    # Write your code here\n",
        "    class_labels = np.unique(y)\n",
        "    # Initialize the entropy\n",
        "    entropy = 0\n",
        "    # Calculate the entropy\n",
        "    for cls in class_labels:\n",
        "        p_cls = len(y[y == cls]) / len(y)\n",
        "        entropy += (-p_cls * np.log2(p_cls))\n",
        "\n",
        "    return entropy\n",
        "    return None"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHXeCu_gdRKN"
      },
      "source": [
        "# Do not change anything in this cell.\n",
        "class Node:\n",
        "\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJv-OeZmdRZT"
      },
      "source": [
        "# Write code in this cell to build your Decision Trees used for Random Forest Classifier\n",
        "#This will be similar to assignment 1, but keep in mind that we will randomly pick \n",
        "#a subset of features for splitting, at then choose the best variable/split-point among those\n",
        "class DecisionTree:\n",
        "\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, num_features=None):\n",
        "        # Initialize the min_split,max_depth, root and num_features are set to None\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.num_features = num_features\n",
        "        self.root = None\n",
        "\n",
        "    # Function to train the decision tree\n",
        "    def fit(self, X, y):\n",
        "        \n",
        "        \"\"\"\n",
        "        Function to train the tree.\n",
        "        X: Features\n",
        "        Y: Target\n",
        "        \"\"\"\n",
        "        self.num_features = X.shape[1] if not self.num_features else min(self.num_features, X.shape[1])\n",
        "        self.root = self.build_tree(X, y)\n",
        "\n",
        "    # Write a function to predict the results for a given dataset, using the tree you built \n",
        "    #and the tree traversal function\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Prediction function to calculate the all the predictions of the matrix of features \n",
        "        provided using make_predictions function\n",
        "        X: Matrix of features\n",
        "        Returns predictions\n",
        "        \"\"\"\n",
        "        return self.tree_traversal(X, self.root)\n",
        "\n",
        "    # build the decision tree\n",
        "    def build_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        This will be a recursive function to build the decision tree.\n",
        "        X: The data that you will be using for your assignment\n",
        "        y: Target\n",
        "        depth: Current depth of the tree\n",
        "        Returns the leaf node\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # stopping criteria should be when depth equals or exceeds max depth\n",
        "        # or there is only one label at the node\n",
        "        # or when n_samples gets smaller than min_samples_split\n",
        "        if (depth > self.max_depth or n_labels == 1 or n_samples <= self.min_samples_split):\n",
        "            leaf_value = self.max_frequency_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "        # select the features **randomly**. Hint: you can use functions like np.random.choice\n",
        "        feature_idxs = np.random.choice(n_features, int(n_features / 10))\n",
        "\n",
        "        # find the best split according to information gain\n",
        "        best_feat, best_thresh = self.optimal_criterion(X, y, feature_idxs)\n",
        "      \n",
        "        # Take the result from the split and recuisively grow the tree\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
        "        left = self.build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
        "        right = self.build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
        "        return Node(best_feat, best_thresh, left, right)\n",
        "\n",
        "    def optimal_criterion(self, X, y, feature_idxs):\n",
        "        \"\"\"\n",
        "        Find the optimal criterion for the split of the tree, using the selected features.\n",
        "        X: dataset\n",
        "        y: target\n",
        "        feature_idxs: randomly selected feature idxs\n",
        "        Return split index and threshold\n",
        "        \"\"\"\n",
        "        # initialize the best gain\n",
        "        best_gain = -1\n",
        "        for feature_index in feature_idxs:\n",
        "            # Find unique threshold values\n",
        "            possible_thresholds = np.unique(X[:, feature_index])\n",
        "            for threshold in possible_thresholds:\n",
        "                #dataset_left, dataset_right = self._split(X[:, feature_index], threshold)\n",
        "                curr_info_gain = self._information_gain(y, X[:, feature_index], threshold)\n",
        "                if curr_info_gain > best_gain:\n",
        "                    best_gain = curr_info_gain\n",
        "                    best_feature_index = feature_index\n",
        "                    best_thres = threshold\n",
        "\n",
        "        # Best feature among the features\n",
        "        split_idx = best_feature_index\n",
        "        # Best threshold for the best feature\n",
        "        split_thresh = best_thres\n",
        "        return split_idx, split_thresh\n",
        "\n",
        "    def _information_gain(self, y, X_column, split_thresh):\n",
        "        \"\"\"\n",
        "        Function to return the information gain\n",
        "        y: target\n",
        "        X: data\n",
        "        split_thresh: threshold for split\n",
        "        Return Information gain\n",
        "        \"\"\"\n",
        "        # parent loss\n",
        "        parent_entropy = entropy(y)\n",
        "\n",
        "        # generate split\n",
        "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
        "\n",
        "        entropy_left = entropy(y[left_idxs]) * len(left_idxs) / len(y)\n",
        "        entropy_right = entropy(y[right_idxs]) * len(right_idxs) / len(y)\n",
        "        information_gain = parent_entropy - entropy_left - entropy_right\n",
        "        return information_gain\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        \"\"\"\n",
        "          (Already implemented)\n",
        "          The split function \n",
        "          X_column: data\n",
        "          split_thresh: threshold value\n",
        "          Return left_idxs, right_idxs\n",
        "          \"\"\"\n",
        "        # Function to split the tree\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "  \n",
        "    def tree_traversal(self, x, node):\n",
        "        \"\"\"\n",
        "          Tree traversal method which returns (one of) the leaf node value\n",
        "          x: data\n",
        "          node: node of the tree\n",
        "          \"\"\"\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "  \n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self.tree_traversal(x, node.left)\n",
        "        return self.tree_traversal(x, node.right)\n",
        "\n",
        "    def max_frequency_label(self, y):\n",
        "        \"\"\"\n",
        "          Determine the target label with maximum frequency. Hint: You can use collections.Counter()\n",
        "          y: target\n",
        "          \"\"\"\n",
        "        most_common = np.argmax(np.bincount(y))\n",
        "        return most_common\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQEcjFnEcrE1"
      },
      "source": [
        "def bootstrap_sample(X, y):\n",
        "    # Function for bootstrap sampling. Hint: use np.random.choice for idxs\n",
        "    idxs = np.random.choice(range(X.shape[0]), X.shape[0])\n",
        "    X_sample, y_samples = X[idxs], y[idxs]\n",
        "    return X_sample, y_samples\n",
        "\n",
        "def most_common_label(y):\n",
        "    \"\"\"\n",
        "      Determine the target label with maximum frequency. Again, you can use collections.Counter()\n",
        "      y: target\n",
        "      \"\"\"\n",
        "    most_common = np.argmax(np.bincount(y))\n",
        "    return most_common"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OZLlPjkc3xk"
      },
      "source": [
        "class RandomForest:\n",
        "    \n",
        "    def __init__(self, n_trees=10, min_samples_split=2,\n",
        "                 max_depth=100, num_features=None):\n",
        "        # Initialize the variabes\n",
        "        self.n_trees = n_trees\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.num_features = num_features\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Create n_tree decision trees using the training data\n",
        "        X: data\n",
        "        y: target\n",
        "        \"\"\"\n",
        "        for i in range(self.n_trees):\n",
        "            X_sample, y_samples = bootstrap_sample(X, y)\n",
        "            tree = DecisionTree(self.min_samples_split, self.max_depth, self.num_features)\n",
        "            tree.fit(X_sample, y_samples)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Write a predict function to make predictions\n",
        "        X: data\n",
        "        Return predictions\n",
        "        \"\"\"\n",
        "        y_pred = []\n",
        "        for i in range(len(X)):\n",
        "            preds = []\n",
        "            for tree in self.trees:\n",
        "                preds.append(tree.predict(X[i]))\n",
        "            y_pred.append(most_common_label(preds))\n",
        "        return y_pred"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKDTnnJPc4Ac"
      },
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "    return accuracy"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9IC3GYFc4J_",
        "outputId": "c94174a2-fe6c-40e9-b7da-592dd2b74c5a"
      },
      "source": [
        "# Do not change anything in this cell\n",
        "import time\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "for n_trees in [1,3,5,10,20]:\n",
        "    clf = RandomForest(n_trees=n_trees, max_depth=10)\n",
        "    start = time.time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_finish = time.time()\n",
        "    y_pred = clf.predict(X_test)\n",
        "    test_finish = time.time()\n",
        "    acc = accuracy(y_test, y_pred)\n",
        "    print (f\"Num of trees : {n_trees} Accuracy: {acc:.2f} Training took {train_finish-start:.2f}s Testing took {test_finish - train_finish:.2f}s\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num of trees : 1 Accuracy: 0.89 Training took 0.36s Testing took 0.00s\n",
            "Num of trees : 3 Accuracy: 0.91 Training took 0.80s Testing took 0.00s\n",
            "Num of trees : 5 Accuracy: 0.92 Training took 1.47s Testing took 0.00s\n",
            "Num of trees : 10 Accuracy: 0.92 Training took 3.10s Testing took 0.00s\n",
            "Num of trees : 20 Accuracy: 0.93 Training took 5.96s Testing took 0.01s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9dOaIkHPj5"
      },
      "source": [
        "## Q2. Clustering (35 points) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5jTKlufHPj6"
      },
      "source": [
        "### K-Means Clustering\n",
        "For this question, you will use the **K-Means** clustering algorithm to cluster the yeast dataset. You will experiment with different k values, and report the Sum of Squared Errors (see below) for each k.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-_TGmwAHPj6"
      },
      "source": [
        "### Sum of Squared Errors (SSE)\n",
        "Assume that we have a clustering for our data $X$ having $K$ clusters. Each cluster is characterized by it’s centroid $C_k$ . The sum of squared error criterion for one cluster can be defined as:\n",
        "$$SSE(X,C =k)= \\sum_{x_i \\in cluster_k}||x_i −C_k||2 $$ \n",
        "then, the overall SSE for the K clusters can be calculated as:\n",
        "\n",
        "$$ SSE(X)= \\sum_{k=1}^K SSE(X,C=k)$$\n",
        "SSE provides a notion of how compact the clusters are and we are going to favor clusters that are more compact, hence we will look for clusterings that minimize SSE. This is an internal clustering quality criterion as we can estimate it using just the cluster labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBiWaEmAHPj7"
      },
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "import math\n",
        "import copy\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtEy6uyGHPj7"
      },
      "source": [
        "# =============================================================================#\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "    data = pd.read_csv(filename, header=None)\n",
        "    dataset = data.values\n",
        "    return dataset"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdjsvUuVHPj7"
      },
      "source": [
        "# =============================================================================#\n",
        "'''\n",
        "Initialize the k centroids.\n",
        "'''\n",
        "def initialize_centroids(k, dataset):\n",
        "    centroids = []\n",
        "    for i in range(k):\n",
        "        centroids.append(dataset[i])\n",
        "    return centroids\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ0C6gVmHPj7"
      },
      "source": [
        "# =============================================================================#\n",
        "def calculate_znk(data, centroids, k, n):\n",
        "    '''\n",
        "    Assign each point to the closest centroid, using a Matrix Z to represent the assignments\n",
        "    '''\n",
        "    znk = numpy.array([[0 for i in range(k)] for j in range(n)])\n",
        "    for i in range(n):\n",
        "        distance_to_cen = []\n",
        "        for j in range(k):\n",
        "            # calculate the distance squared\n",
        "            distance = 0\n",
        "            for m in range(data.shape[1]):\n",
        "                distance += (data[i, m] - centroids[j][m]) ** 2\n",
        "            distance_to_cen.append(numpy.sqrt(distance))\n",
        "        # assign the closer centroid index to be 1\n",
        "        znk[i, numpy.argmin(distance_to_cen)] = 1\n",
        "    return znk"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V79d47xbHPj8"
      },
      "source": [
        "# =============================================================================#\n",
        "def calculate_centroids_and_SSE(k, znk, data, cen):\n",
        "    '''\n",
        "    Calculate the k centroids and the SSE\n",
        "    '''\n",
        "    centroids = []\n",
        "    #clusters = []\n",
        "    for i in range(k):\n",
        "        cluster_i = numpy.where(znk[:, i] == 1)[0]\n",
        "        #clusters.append(len(cluster_i))\n",
        "        centroid = []\n",
        "        for j in range(data.shape[1]):\n",
        "            if len(cluster_i) == 0:\n",
        "                centroid = cen[i]\n",
        "                break\n",
        "            else:\n",
        "                centroid.append(sum(data[cluster_i, j]) / len(cluster_i))\n",
        "        centroids.append(centroid)\n",
        "\n",
        "    total_SSE = 0\n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(k):\n",
        "            # calculate the distance squared\n",
        "            if not centroids[j][0]:\n",
        "                continue\n",
        "            total_SSE += znk[i, j] * calculate_d_squared(data[i, :], centroids[j])\n",
        "    return centroids, total_SSE"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYHbBCionJLp"
      },
      "source": [
        "# This function calculate the squared distance between two vectors(points)\n",
        "def calculate_d_squared(x, centroid):\n",
        "    distance = 0\n",
        "    for i in range(x.shape[0]):\n",
        "        distance += (x[i] - centroid[i]) ** 2\n",
        "    return distance"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIccNpEXHPj8"
      },
      "source": [
        "# =============================================================================#\n",
        "def k_means_algorithm(dataset, k, max_iterations, thresh = 0.01):\n",
        "    '''\n",
        "    Do k-means by iteratively calculating the centroids and assigning the data points to one of the centroids\n",
        "    until either we reach the max_iter or SSE does not decrease by more than the threshold.\n",
        "    Do not use the last column, as they are the labels.\n",
        "    '''\n",
        "    centroids = initialize_centroids(k, dataset)\n",
        "    n = len(dataset)\n",
        "    data = dataset[:, 0:dataset.shape[1] - 1]\n",
        "    cen = []\n",
        "    prev_SSE = math.inf\n",
        "    for i in range(max_iterations):\n",
        "        znk = calculate_znk(data, centroids, k, n)\n",
        "        centroids, total_SSE = calculate_centroids_and_SSE(k, znk, data, centroids)\n",
        "        if prev_SSE - total_SSE <= thresh:\n",
        "            #stop when SSE does not decrease by more than the threshold\n",
        "            return prev_SSE, znk, centroids\n",
        "        cen.append(centroids)\n",
        "        #data.append(total_SSE)\n",
        "        prev_SSE = total_SSE\n",
        "    return prev_SSE, znk, centroids\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "ulRKZhFsHPj8",
        "outputId": "23021255-fc37-49ed-ac3e-24a768c0ceb9"
      },
      "source": [
        "# =============================================================================#\n",
        "# MAIN FUNCTION, DO NOT MODIFY\n",
        "dataset_name = 'yeastData.csv'\n",
        "# Careful, the dataset also consists of labels\n",
        "dataset = load_csv(dataset_name)\n",
        "max_iterations = 1000\n",
        "sum_sq_error = []\n",
        "\n",
        "\n",
        "K = numpy.arange(2, 20, 2)\n",
        "for k in K:\n",
        "    SSE, znk, centroids = k_means_algorithm(dataset, k, max_iterations)\n",
        "    sum_sq_error.append(SSE)\n",
        "    print(\"SSE for k= \", end='')\n",
        "    print(k, end='')\n",
        "    print(' is ', end='')\n",
        "    print(SSE)\n",
        "\n",
        "\n",
        "print(\"-\" * 50)\n",
        "plt.plot(K, sum_sq_error)\n",
        "plt.title(\"SSE against K: Yeast Dataset\")\n",
        "plt.xlabel(\"K\")\n",
        "plt.ylabel(\"SSE\")\n",
        "plt.show()\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SSE for k= 2 is 94.16634322044357\n",
            "SSE for k= 4 is 70.77402837824934\n",
            "SSE for k= 6 is 54.78035253521831\n",
            "SSE for k= 8 is 48.80812672080955\n",
            "SSE for k= 10 is 44.99032745150484\n",
            "SSE for k= 12 is 41.523432060297715\n",
            "SSE for k= 14 is 39.80860317468567\n",
            "SSE for k= 16 is 37.6736423178586\n",
            "SSE for k= 18 is 36.30823794193829\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnMnVot4HPj9"
      },
      "source": [
        "## Q3. Theory (15 points) \n",
        "\n",
        "1.(4 points) Verify that if we minimize the sum-of-squares error between a set of training values {$t_n$} and a single predictive value $t$, then the optimal solution for $t$ is given by the mean of the {$t_n$}. \n",
        "\n",
        "2.(7 points) Consider the K-means algorithm. Show that as a consequence of there being a finite number of possible assignments for the set of discrete indicator variables $z_{nk}$, and that for each such assignment there is a unique optimum for the centroids, the K-means algorithm must converge after a finite number of iterations.\n",
        "\n",
        "3.(4 points) In Q2 we initialize the centroids of the clusters by simply picking the first $k$ data points from the dataset. There are many alternative ways. Provide two other ways of initializing the centroids. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soUBYqq7lE27"
      },
      "source": [
        "1. For the single predicative value,\n",
        "\n",
        "$$SSE = \\sum_{i=1}^n(t_i - t)^2 $$\n",
        "\n",
        "$$SSE = nt^2 + \\sum_{i=1}^nti-2\\sum_{i=1}^nti*t $$\n",
        "\n",
        "$$\\frac {\\partial SSE}{\\partial t} = 2nt - 2\\sum_{i=1}^nti $$\n",
        "\n",
        "$$\\frac {\\partial^2 SSE}{\\partial t^2} = 2n $$\n",
        "\n",
        "We can see that $\\frac {\\partial^2 SSE}{\\partial t^2}=2n>0$, so $\\frac {\\partial SSE}{\\partial t}$ is monotonically increasing.\n",
        "\n",
        "Let $\\frac {\\partial SSE}{\\partial t} = 0$, we have \n",
        "\n",
        "$$\\frac {\\partial SSE}{\\partial t} = 2nt - 2\\sum_{i=1}^nt_i = 0$$\n",
        "\n",
        "$$2nt =2\\sum_{i=1}^nt_i $$\n",
        "\n",
        "$$t =\\frac{1}{n}\\sum_{i=1}^nt_i $$\n",
        "\n",
        "where $\\frac{1}{n}\\sum_{i=1}^nt_i$ is the mean of the ${t_n}$.\n",
        "\n",
        "Since $\\frac {\\partial SSE}{\\partial t}$ is monotonically increasing and $\\frac {\\partial SSE}{\\partial t}=0$ when $t =\\frac{1}{n}\\sum_{i=1}^nt_i $, so SSE has only 1 global minimum point. \n",
        "\n",
        "So the optimal solution for t is the mean of the ${t_n}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U-y6V4OOGT1"
      },
      "source": [
        "2. \n",
        "\n",
        "For K-means clustering, the objective function:\n",
        "\n",
        "$$J=\\sum_{n=1}^N\\sum_{n=1}^Nz_{nk}\\parallel x_n-\\mu_k \\parallel^2$$\n",
        "\n",
        "Since there are finite number of possible assignments fo the $z_{nk}$, each point have a unique minimum with to the ceratain centroid $\\mu$. After a finite number of iterations, the k-means clustering algorithm will converage, when $z_{nk}$ is not changing. If the $z_{nk}$ not changingm, the centroids $\\mu_k$ will keep the same, no changes will make to the objective function J."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLxIOENZ_qSR"
      },
      "source": [
        "3.\n",
        "\n",
        "Method 1 (k-means++):\n",
        "\n",
        "* Choose first centroid randomly from the data points\n",
        "* For each data points, canculate the distance to choosen centroids\n",
        "* Choose new data point using the weighted probability distributation based on the distance calculated in the last step\n",
        "* Repeat last two steps until finsh choosing k centroids.  \n",
        "\n",
        "Method 2:\n",
        "\n",
        "* Calculate the $\\parallel \\textbf{x} \\parallel $ and sort the dataset based on that. \n",
        "* Split the sorted dataset to k pieces\n",
        "* Calculated the mean of each piece as the centroids\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMgLrUCqHPj9"
      },
      "source": [
        ""
      ],
      "execution_count": 54,
      "outputs": []
    }
  ]
}